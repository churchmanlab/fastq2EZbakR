# fastq2EZbakR Tips and Tricks

This page is devoted to a number of small pieces of advice and details about running fastq2EZbakR that you may find useful. In particular, information relevant ot the following topics is provided:

1. [fastq2EZbakR runtime](#runtime)
1. [fastq2EZbakR RAM usage](#ram)
1. [Feature assignment strategies in fastq2EZbakR](#features)

## Runtime<a name="runtime"></a>

Rough estimates for runtimes for a single fastq/bam file from human data, with 20 cores provided to Snakemake, are below:

1. 25 million read fastq file: 1-2 hours + time to index genome if necessary (usually ~0.5-1 hours)
1. 300 million read fastq file: 5-7 hours + time to index genome if necessary
1. 25 million read bam file: ~30 minutes
1. 300 million read bam file: 3-4 hours

In most cases, you will likely provide more than a single fastq/bam file as input. If you follow the steps in [Deployment](deploy.md) exactly, then that means most rules will run serially (i.e., not in parallel). Most steps of the pipeline run on a single sample worth of data at a time, so runtimes in this case will increase approximately linearly with the number of input files. **This is often not the best way to run fastq2EZbakR, or any Snakemake pipeline, though!**

More specifically, if you are running fastq2EZbakR in an environment where it is possible to request separate jobs to run in parallel (e.g., an institution's shared HPC clusters that utilize a scheduler like slurm or qsub), then you can significantly improve fastq2EZbakR runtimes by making full use of the computational resources at your disposal. The [Slurm/Yale HPC](slurm.md) section has some details relevant to this, but those instructions are mostly specific to the Yale HPC. I will try and write some more general documentation for this eventually, but the general idea is that you can provide Snakemake with information about your system's job scheduler. When you do this, Snakemake will be able to effectively run multiple input files in parallel through fastq2EZbakR. This has the potential to nearly flatten the number of input files vs. runtime function, dependent on how many jobs your particular system allows you to have running at the same time. Some documentation to check out relevant to this is:

1. If you are using version 8.0 of Snakemake or later, then there are a number of plugins to support optimized execution on a wide array of systems. Checkout the sidebar of this page for the list of plug-ins : https://snakemake.github.io/snakemake-plugin-catalog/
1. If you are using a version of Snakemake older than 8.0, then the same thing is accomplishable via built-in functionality documented here: https://snakemake.readthedocs.io/en/v7.32.3/executing/cluster.html#cluster-generic
1. If you are using version 7.29 of Snakemake or later, I highly suggest checking out the solution described on [John Blischak's relevant repo](https://github.com/jdblischak/smk-simple-slurm), which uses [profiles](https://snakemake.readthedocs.io/en/stable/executing/cli.html#profiles) to implement a simple and elegant solution. Technically, this repo is specific to systems using the slurm scheduler, but the general architecture of the solution can be applied to other systems as well. **NOTE**: A lot changed in version 8.0 of Snakemake, meaning that this repo had to significantly alter the solution to accomodate newer versions of Snakemake. The pre- and post-8.0 solution is available as separate branches on this repo though (e.g., version 7 solution is [here](https://github.com/jdblischak/smk-simple-slurm/tree/v7)).


## RAM usage<a name="ram"></a>

The RAM requirements of fastq2EZbakR are mainly a function of two things:

1. Genome size 
1. Sequencing depth (i.e., number of total reads in your fastq/bam files)

RAM usage is mostly a function of the former. The main RAM bottlenecks are any rules that call STAR, where the size of data structures used by STAR to efficiently accomplish tasks like alignment are a funtion of genome size but **not** fastq file size. These are (`rule name`: description):

1. `align`: Alignment of reads to genome
1. `index`: Indexing a genome for the aligner
1. `maketdf`: Making colored sequencing tracks, which under the hood calls STAR for creating tracks (not alignment; turns out to also be fairly RAM-intensive).

For human data, RAM usage of these steps is typically in the following ballparks:

1. `align`: ~50 GB of RAM
1. `index`: ~120 GB of RAM. **NOTE**: these only need to be built once for a given genome + annotation combination and can be provided to fastq2EZbakR if you already have them.
1. `maketdf`: ~50 GB of RAM for T-to-C mutation content colored tracks

There is one non-STAR step in fastq2EZbakR that can also be fairly RAM-intensive: `merge_features_and_muts`. This rule joins the counts of mutations created by the `cnt_muts` rule with all of the feature assignment tables generated by their relevant rules. The default strategy uses R and the data.table package, loading each feature assignment table into RAM and joining it with the mutation count table. Whereas RAM-usage for the steps listed above is a mostly flat function of genome size, this step's RAM requirement increases as a function of the number of reads in a given fastq/bam file. RAM-usage can thus range from around ~30 GB for a 25 million read library to ~120 GB for a 300 million read library. This RAM-usage scales approximately linearly with the size of the library. If RAM usage for this step is a challenging bottle neck in your case, you can set the `lowRAM` parameter in your config.yaml file to `TRUE`. This implements a strategy to perform the merging without using hardly any RAM. It requires pre-sorting all of the tables to be merged, and then pre-sorting of all of the merged tables prior to combining them into the final cB file. Thus, the downside of this approach is longer runtime. 

On some occasions, the mutation counting step (`cnt_muts`) can also use a considerable amount of RAM. This is only the case if a large number of SNPs are called in the `call_snps` step of the pipeline. SNPs are only called if you provide a non-empty list to the `controls` parameter of the config.yaml, and by extension is only relevant if you have -label data for calling SNPs. Mutation counting is run in parallel via a somewhat naive strategy of splitting up the bam file and using GNU Parallel to run a custom Python script on the separate bam file chunks in parallel. This means that the entire SNP call text file is loaded into RAM in each GNU Parallel job. Thus, RAM usage in this step is roughly equal to the size of the snp.txt file created by fastq2EZbakR * the number of cores provided to this step. 

## Feature assignment<a name="features"></a>

As documented elsewhere on this website, the real novelty of fastq2EZbakR is the broad array of genomic features to which reads can be assigned. The exhaustive list is:

1. Genes (anywhere)
1. Genes (exons only)
1. Exonic bins (as in DEXSeq)
1. Transcript equivalence classes (e.g., set of transcript isoforms with which a read is fully compatible)
1. Exon-exon junctions (there are two strategies for this; more on this below)
1. Exon-intron junctions

What follows is details regarding the nuance of each of these strategies, and suggestions as to how to most effectively use them:

### Summary

1. Genes (exons only) uses featureCounts and its `--nonOverlap` parameter (documentation here) to find reads that only overlap exonic regions of a gene. This has a minor limitation discussed here that soft-clipped bases are counted as "non-overlapping". Thus, you will either have to use a sufficiently lenient `--nonOverlap` parameter to avoid not assigning soft-clipped reads, or alter alignment parameters to avoid soft-clipping (e.g., in STAR this means setting `--alignEndsType EndToEnd`).
1. Transcript equivalence class assignment uses a custom Python script that parses the transcriptome aligned bam file produced by STAR. If your annotation includes genes on different strands that partially overlap, transcripts from both of these genes can be improperly included by this
assignment strategy. [EZbakR](https://github.com/isaacvock/EZbakR), the tool most likely being used in conjunction with this strategy, can be provided a table of gene-to-transcript assignments to filter out such misassignments. In particular, see the `EstimateIsoformFractions()` function's `gene_to_transcript` parameter.
1. Exonic bins assignment uses featureCounts, and will assign reads to exonic bins even if a part of the read overlaps with purely intronic regions of a gene. Thus, this is best paired with either Genes (exons only) assignment or Transcript equivalence class assignment, so that reads form pre-RNA can be filtered out as necessary.
1. There are two exon-exon junction assignment strategies, one under the name `junctions` in the config.yaml file, and the other under the name `eej`. The former is only compatible with bam files produced by STAR containing the custom jI and jM tags. The latter is a less accurate, and still experimental option using featureCounts that is compatible with any bam file.
1. The exon-intron junction assignment strategy, like `eej`, is stil experimental. The accuracy of assignments by these two strategies can be bolstered by combining them with transcript equivalence class assignment (to filter out purely exonic reads that are misassigned to an exon-intron junction) and the always present sj column of the final cB, which tracks whether or not a gap existed in the reads alignment (e.g., orthogonal evidence for any exon-exon junction assignments).
1. Several of the assignment strategies can see a read assigned to multiple features (e.g., a read overlapping multiple distinct exonic bins). In these cases, the relevant entry in the cB table will look like "featureID1+featureID2+...+featureIDN", where "featureIDi" is the name of the ith feature of the relevant type that the read was assigned to. Reads not assigned to any feature are given a string of "__no_feature".